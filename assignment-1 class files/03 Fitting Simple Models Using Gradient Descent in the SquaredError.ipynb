{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Simple Models Using Gradient Descent in the Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $N$ observations, $\\xv_n$, for $n=1,\\ldots,N$, and target values,\n",
    "$t_n$, for $n=1,\\ldots,N$, what is the simplest model,\n",
    "$g(\\xv)$, you can think of?\n",
    "\n",
    "$$\n",
    "g(\\xv) = 0\n",
    "$$\n",
    "\n",
    "or maybe\n",
    "\n",
    "$$\n",
    "g(\\xv) = c\n",
    "$$\n",
    "\n",
    "What is next simplest model?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   g(\\xv;\\wv) &= w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_D x_D \\\\\n",
    "   &= w_0 + \\sum_{i=1}^D w_i x_i \\\\\n",
    "   & = \\sum_{i=0}^D w_i x_i \\mbox{, where } x_0 = 1 \\\\\n",
    "   &= \\wv^T \\xv\\\\\n",
    "   &= \\xv^T \\wv\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "  * This is nice because it is linear in the parameters $\\wv$; optimizations based on derivatives might be solvable analytically.\n",
    "  * This is not so nice, because it is also linear in the inputs, $\\xv$; greatly limits the complexity of the model.\n",
    "  *  But, a model linear in the inputs might be the best you can do for many cases, such as a sparsely sampled distribution, process, population, thing...whatever it is you want to model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data Samples with a Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Springs](http://www.cs.colostate.edu/~anderson/cs545/notebooks/springs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force exerted by spring is proportional to its length. The potential\n",
    "energy stored in the spring is proportional to the square of its length.\n",
    "Say we want the rod to settle at position that minimizes the potential\n",
    "energy in the springs.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{n=1}^N (t_n - g(\\xv_n;\\wv))^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If $g$ is an affine (linear + constant) function of $x$,\n",
    "$$\n",
    "    g(\\xv;\\wv) = w_0 + w_1 x\n",
    "$$\n",
    "with parameters $\\wv = (w_0, w_1)$, which parameter values give best fit?\n",
    "$$\n",
    "    \\wv_{\\mbox{best}} = \\argmin{\\wv} \\sum_{n=1}^N (t_n - g(x_n ; \\wv))^2\n",
    "$$\n",
    "\n",
    "Set derivative (gradient) with respect to $\\wv$ to zero and\n",
    "solve for $\\wv$.  Let's do this with matrices. \n",
    "\n",
    "The matrix formulas are a bit simpler if we assume that $w_0$ is multipled by the constant 1, and that $x_{i, 0}$, the first component of sample $i$, is the constant 1.\n",
    "\n",
    "Collect all targets into matrix $T$ and $x$ samples into matrix\n",
    "$X$. ($N$=number samples, $D$=sample dimensionality)\n",
    "$$\n",
    "  \\begin{align*}\n",
    "    T &= \\begin{bmatrix}\n",
    "      t_1 \\\\ t_2 \\\\ \\vdots \\\\ t_N\n",
    "    \\end{bmatrix} \\\\\n",
    "    X &= \\begin{bmatrix}\n",
    "      x_{1,0} & x_{1,1} & x_{1,2} & \\dotsc & x_{1,D} \\\\\n",
    "      x_{2,0} & x_{2,1} & x_{2,2} & \\dotsc & x_{2,D} \\\\\n",
    "      \\vdots \\\\\n",
    "      x_{N,0} & x_{N,1} & x_{N,2} & \\dotsc & x_{N,D}\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\wv &= \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{bmatrix}\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Collection of all differences is $T - X\\wv$, which is an $N \\times\n",
    "1$ matrix.  To form the square of all values and add them up, just\n",
    "do a dot product $(T-X\\wv)^T (T-X\\wv)$.  This only works if the value we are predicting is a scalar, which means $T$ is a column matrix.  If we want to predict more than one value for each sample, $T$ will have more than one column.  Let's continue with the derivation assuming $T$ has $K$ columns, meaning we want a linear model with $K$ outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best value for $\\wv$, we take the derivative of the sum of squared error objective, set it equal to 0 and solve for $\\wv$. Here $\\xv_n$ is one sample as a column vector, so it must be transposed to a row vector before being multiplied by the column vector $\\wv$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv))^2}{\\partial \\wv} &= -2 \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv) \\frac{\\partial g(\\xv_n;\\wv)}{\\partial \\wv}\\\\\n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\frac{\\partial \\xv_n^T \\wv}{\\partial \\wv}\\\\\n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\xv_n^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here's where we get the benefit of expressing the $\\xv_n$ and $t_n$ samples as matrices. The sum can be performed with a dot product:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (\\tv_n - g(\\xv_n;\\wv))^2}{\\partial \\wv} \n",
    "&= -2 \\sum_{n=1}^N (\\tv_n - \\xv_n^T \\wv) \\xv_n^T\\\\\n",
    "&= -2 \\Xv^T (\\Tv - \\Xv \\wv)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Check the sizes and shapes of each matrix in the last equation above.\n",
    "\n",
    "Now we can set this equal to zero and solve for $\\wv$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "-2 \\Xv^T (\\Tv - \\Xv \\wv) &= 0\\\\\n",
    "\\Xv^T (\\Tv - \\Xv \\wv) &= 0\\\\\n",
    "\\Xv^T \\Tv &= \\Xv^T \\Xv \\wv\\\\\n",
    "\\wv &= (\\Xv^T \\Xv)^{-1} \\Xv^T \\Tv\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, in python\n",
    "\n",
    "    w = np.linalg.inv(X.T @ X), X.T @ T)\n",
    "\n",
    "or, you may use the *solve* function that assumes $\\Xv^T \\Xv$ is full rank (no linearly dependent columns),\n",
    "\n",
    "    w = np.linalg.solve(X.T @ X, X.T @ T)\n",
    "\n",
    "or, better yet, use the *lstsq* function that does not make that assumption. \n",
    "\n",
    "    w = np.linalg.lstsq(X.T @ X, X.T @ T))\n",
    "    \n",
    "The ```lstsq``` and ```solve``` functions can be written with simpler arguments, like\n",
    "\n",
    "    w = np.linalg.lstsq(X, T))\n",
    "    \n",
    "because they are designed to find the value of $\\wv$ that minimized the squared error between $X \\wv$ and $T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you have thousands or millions of samples?  $X$ and $T$\n",
    "can be quite large. To avoid dealing with matrix operations on huge\n",
    "matrices, we can derive a sequential algorithm for finding $\\wv$ by\n",
    "using the fact that a derivative of a sum is the sum of the\n",
    "derivatives.  We will now express this derivative as a gradient, which is a vector or matrix of derivatives.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g(\\xv_n, \\wv) &= w_0 + w_1 x_{n,1} + w_2 x_{n,2} + \\cdots + w_D x_{n,D} = \\xv_n^T \\wv\\\\\n",
    "E(\\Xv, \\Tv, \\wv) &= \\sum_{n=1}^N (t_n - g(\\xv_n, \\wv))^2\\\\\n",
    "\\nabla_\\wv E(\\Xv, \\Tv, \\wv) &= \\nabla_\\wv \\left ( \\sum_{n=1}^N (t_n - g(\\xv_n, \\wv))^2 \\right )\\\\\n",
    "&= \n",
    "\\sum_{n=1}^N \\nabla_\\wv (t_n - g(\\xv_n, \\wv))^2\\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(\\xv_n, \\wv)) \\nabla_\\wv (t_n - g(\\xv_n, \\wv)) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(\\xv_n, \\wv)) (-1) \\nabla_\\wv g(\\xv_n, \\wv) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(\\xv_n, \\wv)) (-1) \\nabla_\\wv (\\xv_n^T \\wv) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(\\xv_n, \\wv)) (-1) \\xv_n \\\\\n",
    "&= \n",
    "-2 \\sum_{n=1}^N (t_n - g(\\xv_n, \\wv))  \\xv_n \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of summing over all $N$ samples, what if we\n",
    "just update $\\wv$ after each sample based on the gradient of $E$ for that sample?  The gradient for a \n",
    "sample $n$ can be\n",
    "considered as a limited, or noisy, sample of the true gradient.\n",
    "Thus, we can take a small step in the direction of the negative gradient to try\n",
    "to bring a current guess at the weight vector, $\\wv^{(k)}$, on\n",
    "iteration $k$ to a new value, $\\wv^{(k+1)}$, on iteration $k+1$ that is closer to a value that reduces the overall error. This kind of update is called \"stochastic approximation\".\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\wv^{(k+1)} &= \\wv^{(k)} - (-2) \\rho (t_n - g(\\xv_n, \\wv)) \\xv_n\\\\\n",
    " &= \\wv^{(k)} + \\rho (t_n - g(\\xv_n, \\wv)) \\xv_n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For this sequential algorithm to converge, $\\rho$ must decrease with\n",
    "each iteration, not too fast but not too slow. \n",
    "\n",
    "This algorithm is called the least mean squares (LMS) algorithm\n",
    "developed by Widrow and Hoff.  It is now often referred to as the\n",
    "**''stochastic gradient descent'' algorithm, or SGD.**\n",
    "\n",
    "If we have two output variables $t_n$ is no longer a scalar.  How do we deal with that?  Well, to predict two variables, we need two linear models.  We can do this by changing $\\wv$ from a single column matrix to a two-column matrix.  The first column could contain the weights used to predict mpg, and the second column could contain weights to predict horsepower.  Now our linear model is\n",
    "\n",
    "$$ g(\\xv_n, \\wv) = \\xv_n^T \\wv$$\n",
    "\n",
    "Humm, no change here!  This is the beauty of using matrix math.  The input vector $\\xv_n$ is dotted with each of the two columns of $\\wv$, resulting in two values, or a two-component resulting vector, giving the predictions for mpg and horsepower.\n",
    "\n",
    "What changes do we need to make to the SGD update formula?  What else must we modify, other than $\\wv$?  For each sample, $n$, we must specify two target values, for mpg and horsepower.  So $t_n$ is no longer a scalar, but now has two values in a vector, or $\\tv_n$.  To update the weights $\\wv$ we must multiply each error by each input component. This does sound like a double loop.  Well, in the last equation above we already used matrix math and ```numpy``` broadcasting once in\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\wv^{(k+1)} &= \\wv^{(k)}  + \\rho \\; (t_n - g(\\xv_n, \\wv)) \\; \\xv_n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "to remove the loop over all of the components in $\\wv_n$ and $\\xv_n$.  Now we will use broadcasting again to remove a loop over target components, in $\\tv_n$.  We must take care to make sure the matrices are of the right shape in the matrix operations, and that the resulting matrix is the correct shape for $\\wv$.  Here we follow the convention that vectors are column vectors.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\wv^{(k+1)} &= \\wv^{(k)}  + \\rho \\; \\xv_n \\; (\\tv_n^T - g(\\xv_n, \\wv))) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's see, $\\rho$ is a scalar, $\\xv_n$ is $D+1\\times 1$, a column vector with $D+1$ components (counting the constant 1), $\\tv_n$ is $K\\times 1$ if we have $K$ outputs,\n",
    "so $\\tv_n^T$ is $1\\times K$  and $g(\\xv_n, \\wv) = \\xv_n^T \\wv$ is also $1\\times K$.  Stringing these dimensions together in the  calculation gives us $(D+1\\times 1) (1\\times K)$ which results in $D+1\\times K$ exactly the correct shape for our weight matrix $\\wv$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the update to the weight matrix for the $n^{th}$ sample is just\n",
    "\n",
    "     w += rho * X1[n:n + 1, :].T * (T[n:n + 1, :] - predicted)\n",
    "     \n",
    "The long, boring, non-matrix way to update each element of `w` would look like\n",
    "\n",
    "     nOutputs = T.shape[1]\n",
    "     nInputs = X1.shape[1]\n",
    "     for k in range(nOutputs):\n",
    "         for i in range(nInputs):\n",
    "             w[i,k] += rho * X1[n:n + 1, i] * (T[n:n + 1, k] - predicted[k])\n",
    "\n",
    "So many lines of code can lead to more bugs!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of SGD in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:41.069416Z",
     "start_time": "2021-07-29T18:13:40.549315Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, clear_output  # for the following animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some silly data to play with.  Make 100 samples of random $x$ values between 0 and 10, and assign the target for each sample to be $2 - 0.1 X + (X - 6)^2 + \\epsilon$, where $\\epsilon$ is a bit of noise as a random value from a Normal distribution with mean 0 and standard deviation 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:44.230811Z",
     "start_time": "2021-07-29T18:13:44.221871Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "X = np.random.uniform(0, 10, (n_samples, 1))\n",
    "T = 2 - 0.1 * X + 0.05 * (X - 6)**2 + np.random.normal(0, 0.1, (n_samples,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:48.236595Z",
     "start_time": "2021-07-29T18:13:47.728720Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:52.132489Z",
     "start_time": "2021-07-29T18:13:51.807280Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, T, 'o');  # ;  suppresses one line of output like [<matplotlib.lines.Line2D at 0x7f9657fa0c40>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think we can fit a linear model to this data?\n",
    "\n",
    "First, let's modify the $X$ input matrix to include an initial column of constant 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:57.308406Z",
     "start_time": "2021-07-29T18:13:57.291597Z"
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "X1.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:13:59.012072Z",
     "start_time": "2021-07-29T18:13:58.998227Z"
    }
   },
   "outputs": [],
   "source": [
    "X1[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find good weights by adjusting them to follow the negative gradient of the squared error function using the stochastic gradient descent (SGD) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:14:10.733426Z",
     "start_time": "2021-07-29T18:14:10.688649Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_samples = X1.shape[0]  # number of rows in data equals the number of samples\n",
    "\n",
    "W = np.zeros((2, 1))                # initialize the weights to zeros\n",
    "for epoch in range(10):             # train for this many epochs, or passes through the data set\n",
    "    for n in range(n_samples):\n",
    "        Y = X1[n:n + 1, :] @ W      # predicted value, y, for sample n\n",
    "        error = (T[n:n + 1, :] - Y)  # negative gradient of squared error\n",
    "        \n",
    "        # update weights by fraction of negative derivative of square error with respect to weights\n",
    "        W -=  learning_rate * -2 * X1[n:n + 1, :].T * error  \n",
    "        \n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this linear model (defined by these resulting weights) fits the data.  To do so, we can plot the model's predictions on top of the plot of actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:14:16.959312Z",
     "start_time": "2021-07-29T18:14:16.683806Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, T, 'o', label='Data')\n",
    "plt.plot(X, X1 @ W, 'ro', label='Predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's animate each step by drawing the predictions made by the linear model as weights are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:14:48.277632Z",
     "start_time": "2021-07-29T18:14:48.259637Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(rho, n_epochs, stepsPerFrame=10):\n",
    "\n",
    "    # Initialize weights to all zeros\n",
    "    # For this demonstration, we will have one variable input. With the constant 1 input, we have 2 weights.\n",
    "    W = np.zeros((2,1))\n",
    "\n",
    "    # Collect the weights after each update in a list for later plotting. \n",
    "    # This is not part of the training algorithm\n",
    "    ws = [W.copy()]\n",
    "\n",
    "    # Create a bunch of x values for plotting\n",
    "    xs = np.linspace(0, 10, 100).reshape((-1, 1))\n",
    "    xs1 = np.insert(xs, 0, 1, axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # For each pass (one epoch) through all samples ...\n",
    "    for iter in range(n_epochs):\n",
    "        # For each sample ...\n",
    "        for n in range(n_samples):\n",
    "        \n",
    "            # Calculate prediction using current model, w.\n",
    "            #    n:n+1 is used instead of n to preserve the 2-dimensional matrix structure\n",
    "            Y = X1[n:n + 1,:] @ W\n",
    "            \n",
    "            # Update w using negative gradient of error for nth sample\n",
    "            W += rho * X1[n:n + 1, :].T * (T[n:n + 1, :] - Y)\n",
    "            \n",
    "            # Add new w to our list of past w values for plotting\n",
    "            ws.append(W.copy())\n",
    "        \n",
    "            if n % stepsPerFrame == 0:\n",
    "                fig.clf()\n",
    "\n",
    "                # Plot the X and T data.\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(X, T, 'o', alpha=0.6, label='Data')\n",
    "                plt.plot(X[n,0], T[n], 'ko', ms=10, label='Last Trained Sample')\n",
    "\n",
    "                # Plot the output of our linear model for a range of x values\n",
    "                plt.plot(xs, xs1 @ W, 'r-', linewidth=5, label='Model')\n",
    "                plt.xlabel('$x$')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.xlim(0, 10)\n",
    "                plt.ylim(0, 5)\n",
    "\n",
    "                # In second panel plot the weights versus the epoch number\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(np.array(ws)[:, :, 0])\n",
    "                plt.xlabel('Updates')\n",
    "                plt.xlim(0, n_epochs * n_samples)\n",
    "                plt.ylim(-1, 3)\n",
    "                plt.legend(('$w_0$', '$w_1$'))\n",
    "        \n",
    "                clear_output(wait=True)\n",
    "                display(fig)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:15:36.638729Z",
     "start_time": "2021-07-29T18:14:51.368861Z"
    }
   },
   "outputs": [],
   "source": [
    "run(0.01, n_epochs=1, stepsPerFrame=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T18:17:07.358769Z",
     "start_time": "2021-07-29T18:15:40.074567Z"
    }
   },
   "outputs": [],
   "source": [
    "run(0.01, n_epochs=20, stepsPerFrame=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Fixed Nonlinear Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models we have been buildling are linear in the parameters $\\wv$\n",
    "and linear in the attributes (features) of the samples.  We can make\n",
    "models that are nonlinear in the attributes by adding nonlinear\n",
    "functions of the original features.  \n",
    "\n",
    "Say we have a single feature for each sample.  Our data matrix is\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    " X &= \\begin{bmatrix}\n",
    "       x_0\\\\\n",
    "       x_1\\\\\n",
    "       \\vdots \\\\\n",
    "       x_N\n",
    "       \\end{bmatrix}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "We can add other powers of each $x$ value, say up to the fourth power.\n",
    "$$\n",
    "\\begin{alignat*}{1}\n",
    " X &= \\begin{bmatrix}\n",
    "       x_0 & x_0^2 & x_0^3 & x_0^4\\\\\n",
    "       x_1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "       \\vdots \\\\\n",
    "       x_N & x_N^2 & x_N^3 & x_N^4\\\\\n",
    "       \\end{bmatrix}\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "This is simple to do in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1.1, 2.3, 6.2, 4.2]).reshape(-1, 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** range(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, we can raise X to an exponent of 0 to include the initial column of constant 1 values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** range(0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if we have more than 1 input feature in X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(5, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ** np.array([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rats!   Not working. We must assemble our powers of X one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X ** p for p in [1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([X ** p for p in [1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_powers(X, max_power):\n",
    "    return np.hstack([X ** p for p in range(1, max_power + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Which of these powers of $x$ are useful?  Looking at the magnitudes of\n",
    "the weights is helpful, as long as the input features have similar ranges.  A typical way to force this is to *standardize* the inputs.  This usually means the we subtract the mean of each feature from the values of that feature, then divide by its standard deviation.  So we must first calculate the column means and column standard deviations of $X$.  No worries, `numpy` can do that!\n",
    "\n",
    "        Xst = (X - np.mean(X, axis=0)) / np.std(X, axis=0)      \n",
    "We will be standardizing inputs like this throughout the semester.\n",
    "\n",
    "Now, if we train multiple\n",
    "models from multiple [bootstrap samples](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) of the training data, we can\n",
    "compute confidence intervals of the weight values.  If zero is\n",
    "not included in the range of\n",
    "weight values specified by a weight's 90% lower and\n",
    "upper confidencce limit, then we can say that we are 90% certain that\n",
    "the value of this weight is not zero.  If the range does include zero,\n",
    "the corresponding feature is probably one that is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, T, n_epochs, rho):\n",
    "    \n",
    "    means = X.mean(0)\n",
    "    stds = X.std(0)\n",
    "    # Replace stds of 0 with 1 to avoid dividing by 0.\n",
    "    stds[stds == 0] = 1\n",
    "    Xst = (X - means) / stds\n",
    "    \n",
    "    Xst = np.insert(Xst, 0, 1, axis=1)  # Insert column of 1's as first column in Xst\n",
    "    \n",
    "    # n_samples, n_inputs = Xst.shape[0]\n",
    "    n_samples, n_inputs = Xst.shape\n",
    "    \n",
    "    # Initialize weights to all zeros\n",
    "    W = np.zeros((n_inputs, 1))  # matrix of one column\n",
    "    \n",
    "    # Repeat updates for all samples for multiple passes, or epochs,\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Update weights once for each sample.\n",
    "        for n in range(n_samples):\n",
    "        \n",
    "            # Calculate prediction using current model, w.\n",
    "            #    n:n+1 is used instead of n to preserve the 2-dimensional matrix structure\n",
    "            Y = Xst[n:n + 1, :] @ W\n",
    "            \n",
    "            # Update w using negative gradient of error for nth sample\n",
    "            W += rho * Xst[n:n + 1, :].T * (T[n:n + 1, :] - Y)\n",
    "                \n",
    "    # Return a dictionary containing the weight matrix and standardization parameters.\n",
    "    return {'W': W, 'means' : means, 'stds' :stds, 'max_power': max_power}\n",
    "\n",
    "def use(model, X):\n",
    "    Xst = (X - model['means']) / model['stds']\n",
    "    Xst = np.insert(Xst, 0, 1, axis=1)\n",
    "    Y = Xst @ model['W']\n",
    "    return Y\n",
    "\n",
    "def rmse(A, B):\n",
    "    return np.sqrt(np.mean( (A - B)**2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make a simple function of $x$. Let's try $f(x) = -1 + 0.1 x^2 - 0.02 x^3 + 0.5 n$, where $n$ is from a standard Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 40\n",
    "training_fraction = 0.8\n",
    "n_models = 1000\n",
    "confidence = 90 # percent\n",
    "max_power = 1  # linear model\n",
    "\n",
    "X = np.hstack((np.linspace(0, 3, num=n_samples),\n",
    "               np.linspace(6, 10, num=n_samples))).reshape(2 * n_samples, 1)\n",
    "T = -1 + 0 * X + 0.1 * X**2 - 0.02 * X**3 + 0.5 * np.random.normal(size=(2 * n_samples, 1))\n",
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, T, '.-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into training and testing sets, randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(7.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = X.shape[0]\n",
    "row_indices = np.arange(n_rows)\n",
    "np.random.shuffle(row_indices)\n",
    "n_train = round(n_rows * training_fraction)\n",
    "n_test = n_rows - n_train\n",
    "\n",
    "Xtrain = X[row_indices[:n_train], :]\n",
    "Ttrain = T[row_indices[:n_train], :]\n",
    "Xtest = X[row_indices[n_train:], :]\n",
    "Ttest = T[row_indices[n_train:], :]\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xtrain[:, 0], Ttrain, 'o', label='Train')\n",
    "plt.plot(Xtest[:, 0], Ttest, 'ro', label='Test')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make models based on bootstrap samples of training data.  `models` will be list of models, one for each bootstrap sample.\n",
    "\n",
    "For each bootstrap sample of our training data we will randomly choose `n_train` samples **with replacement**.  The following code cell illustrates how to create 20 bootstrap samples, each with 10 samples.  The bootstrap samples are defined as row indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(list(range(11)), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power = 1\n",
    "Xtrain = X[row_indices[:n_train], :]\n",
    "Xtest = X[row_indices[n_train:], :]\n",
    "Xtrain = make_powers(Xtrain, max_power)\n",
    "Xtest = make_powers(Xtest, max_power)\n",
    "\n",
    "n_epochs = 1000\n",
    "rho = 0.01\n",
    "\n",
    "n_models = 10\n",
    "\n",
    "models = []\n",
    "for model_i in range(n_models):\n",
    "    train_rows = np.random.choice(list(range(n_train)), n_train)\n",
    "    Xtrain_boot = Xtrain[train_rows, :]\n",
    "    Ttrain_boot = Ttrain[train_rows, :]\n",
    "    model = train(Xtrain_boot, Ttrain_boot, n_epochs, rho)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply all of the models to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use(models[0], Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_all = []\n",
    "for model in models:\n",
    "    Y_all.append( use(model, Xtest) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `numpy.array` for all outputs of all models so we can easily calculate the mean for each test sample over all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Y_all).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use `numpy.squeeze` to wring out the \"unused\" dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Y_all).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_all = np.array(Y_all).squeeze().T  # I like putting each model's output in a column, so `Y_all` now has each model's output for a sample in a row.\n",
    "Ytest = np.mean(Y_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test = np.sqrt(np.mean((Ytest - Ttest)**2))\n",
    "print(f'Test RMSE is {RMSE_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 200\n",
    "Xplot = np.linspace(0, 12.5, n_plot).reshape(n_plot, 1)\n",
    "Xplot_powers = make_powers(Xplot, max_power)\n",
    "Ys = []\n",
    "for model in models:\n",
    "    Yplot = use(model, Xplot_powers)\n",
    "    Ys.append(Yplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Ys).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys = np.array(Ys).squeeze().T\n",
    "Ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(Xtrain[:, 0], Ttrain, 'o')\n",
    "plt.plot(Xtest[:, 0], Ttest, 'o')\n",
    "plt.plot(Xplot, Ys, alpha=0.5);\n",
    "plt.ylim(-14, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do again with nonlinear terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power = 6\n",
    "Xtrain = X[row_indices[:n_train], :]\n",
    "Xtest = X[row_indices[n_train:], :]\n",
    "Xtrain = make_powers(Xtrain, max_power)\n",
    "Xtest = make_powers(Xtest, max_power)\n",
    "\n",
    "n_epochs = 2000\n",
    "rho = 0.05\n",
    "\n",
    "n_models = 100 \n",
    "\n",
    "models = []\n",
    "for model_i in range(n_models):\n",
    "    train_rows = np.random.choice(list(range(n_train)), n_train)\n",
    "    Xtrain_boot = Xtrain[train_rows, :]\n",
    "    Ttrain_boot = Ttrain[train_rows, :]\n",
    "    model = train(Xtrain_boot, Ttrain_boot, n_epochs, rho)\n",
    "    models.append(model)\n",
    "    print(f'Model {model_i}', end=' ')\n",
    "    \n",
    "n_plot = 200\n",
    "Xplot = np.linspace(0, 12.5, n_plot).reshape(n_plot, 1)\n",
    "Xplot_powers = make_powers(Xplot, max_power)\n",
    "Ys = []\n",
    "for model in models:\n",
    "    Yplot = use(model, Xplot_powers)\n",
    "    Ys.append(Yplot)\n",
    "\n",
    "Ys = np.array(Ys).squeeze().T\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(Xtrain[:, 0], Ttrain, 'o')\n",
    "plt.plot(Xtest[:, 0], Ttest, 'o')\n",
    "plt.plot(Xplot, Ys, alpha=0.5);\n",
    "plt.ylim(-14, 2);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to evaluate the significance of each input by considering the weights on each input across the models.  First, let's collect the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Ws = [model['W'] for model in models]\n",
    "len(all_Ws), all_Ws[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_Ws).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Ws = np.array(all_Ws).squeeze()\n",
    "all_Ws.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ignore the weight on the constant input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Ws = all_Ws[:, 1:]\n",
    "all_Ws.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we must sort the weight values independently for each input to find the 10% and 90% quantile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.random.randint(-10, 10, size=50).reshape(10, 5)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(Z, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Ws = np.sort(all_Ws, axis=0)\n",
    "low_high = all_Ws[[9, 89], :].T\n",
    "low_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(low_high):\n",
    "    print(f'Power {i + 1:2} Low {row[0]:6.2f} High {row[1]:6.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's try some other data.  Here is some data related to the design of hulls on yachts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\n",
    "!head yacht_hydrodynamics.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('yacht_hydrodynamics.data')\n",
    "\n",
    "T = data[:, -1:]\n",
    "X = data[:, :-1]\n",
    "Xnames = ['Center of Buoyancy', 'Prismatic coefficient', 'Length-displacement ratio', 'Beam-draught ratio',\n",
    "          'Length-beam ratio', 'Froude number']\n",
    "Tname = 'Resistance'\n",
    "X.shape, T.shape, Xnames, Tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.plot(X[:, i] ,T, '.')\n",
    "    plt.ylabel(Tname)\n",
    "    plt.xlabel(Xnames[i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:100, :])\n",
    "plt.plot(T[:100, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(X, T, n_epochs=1000, rho=0.01)\n",
    "predict = use(model, X)\n",
    "print(rmse(predict, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T)\n",
    "plt.plot(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T, predict, 'o')\n",
    "plt.plot([0, 50], [0, 50],  'r-')\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humm...that last variable, the Froude number, looks like its square root might be more linearly related to resistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,-1], T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,-1]**2, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,-1]**4, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,-1]**8, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = make_powers(X, 5)\n",
    "model = train(Xp, T,  n_epochs=1000, rho=0.01)\n",
    "predict = use(model, Xp)\n",
    "print(rmse(predict, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T)\n",
    "plt.plot(predict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T, predict, 'o')\n",
    "plt.plot([0, 50], [0, 50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "plt.plot(T[:n])\n",
    "plt.plot(predict[:n]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe higher powers would work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for max_power in range(1, 20):\n",
    "    Xp = make_powers(X, max_power)\n",
    "    model = train(Xp, T, n_epochs=1000, rho=0.001)\n",
    "    error = rmse(use(model, Xp), T)\n",
    "    print(f'{max_power=} {error=}')\n",
    "    result.append([max_power, error])\n",
    "result = np.array(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result[:,0],result[:,1],'o-')\n",
    "plt.xlabel('Exponent of X')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = make_powers(X, 6)\n",
    "predict = use(train(Xp, T, n_epochs=1000, rho=0.01), Xp)\n",
    "\n",
    "plt.plot(T)\n",
    "plt.plot(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T, predict, 'o')\n",
    "plt.plot([0, 50], [0, 50])\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted');"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
