{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\Dv}{\\mathbf{D}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we saw how transforming the inputs using fixed nonlinear functions does result in models that are nonlinear in the inputs.  However, there is no general way to know which nonlinear functions to use. \n",
    "\n",
    "A more general approach is to pick the form of a nonlinear function that is controlled by parameters, or weights,  whose values control the actual shape of the function. There are many possibilities for constructing such functions. Let's add some desireable properties:\n",
    "\n",
    "   1. Computationally simple.\n",
    "   2. For initial, small, weight values the function is close to linear.  As the weight magnitudes grow, the function becomes increasingly nonlinear.\n",
    "   3. The derivative of the function is computationally simple.\n",
    "   4. The magnitude of the derivative decreases as the weight magnitudes grows, perhaps asymptotically.\n",
    "   5. The maximum value of the magnitude of the derivative is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:06.502506Z",
     "start_time": "2022-08-26T17:42:05.745524Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a linear weighted sum, $s = \\wv^T \\xv$.  So, for a particular input sample $\\xv$, $s$ is small if the magnitudes of the weights of $\\wv$ are near zero.  As the weight magnitudes increase, the magnitude of $s$ increases.  So let's try to construct a function of $s$ in the shape of the derivative that we want.\n",
    "\n",
    "We know that using $s$ as an exponent can make functions asymptotically decrease to zero.  Let's use base $e$, in case we deal with natural logarithms in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:08.988425Z",
     "start_time": "2022-08-26T17:42:08.724815Z"
    }
   },
   "outputs": [],
   "source": [
    "s = np.linspace(-10, 10, 100)\n",
    "plt.plot(s, np.exp(-s));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we are constructing a derivative, so we want to limit the maximum value of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:11.059550Z",
     "start_time": "2022-08-26T17:42:10.971438Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(s, 1/(1 + np.exp(-s)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good.  Does what we want as $s$ grows more negative, but we also want to bring this function down to zero as $s$ becomes more positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:12.062831Z",
     "start_time": "2022-08-26T17:42:11.974460Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(s, 1 - 1/(1 + np.exp(-s)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay.  Now how can we combine these.  How about just multiply them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:12.959627Z",
     "start_time": "2022-08-26T17:42:12.871022Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(s, 1/(1 + np.exp(-s)) * (1 - 1/(1 + np.exp(-s))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  Looks great.  It asymptotes to zero as the magnitude of $s$ grows and the maximum value is limited at 0.25.\n",
    "\n",
    "How simple is it?  Well, there are common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:13.725989Z",
     "start_time": "2022-08-26T17:42:13.638380Z"
    }
   },
   "outputs": [],
   "source": [
    "y = 1  / (1 + np.exp(-s))\n",
    "plt.plot(s, y * (1 - y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, if we like this derivative, all we have to do is integrate it to get an activation function.\n",
    "\n",
    "Let's use python's [`sympy`](https://www.sympy.org/en/index.html) package to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:16.807503Z",
     "start_time": "2022-08-26T17:42:14.950919Z"
    }
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "sympy.init_printing(use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:18.418344Z",
     "start_time": "2022-08-26T17:42:17.719930Z"
    }
   },
   "outputs": [],
   "source": [
    "sS = sympy.Symbol('s')\n",
    "sS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:18.423248Z",
     "start_time": "2022-08-26T17:42:18.420541Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:35.474236Z",
     "start_time": "2022-08-26T17:42:35.069930Z"
    }
   },
   "outputs": [],
   "source": [
    "yS = sS ** 4\n",
    "yS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:40.278647Z",
     "start_time": "2022-08-26T17:42:39.644091Z"
    }
   },
   "outputs": [],
   "source": [
    "sympy.diff(yS, sS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:43.206165Z",
     "start_time": "2022-08-26T17:42:42.784645Z"
    }
   },
   "outputs": [],
   "source": [
    "yS = 1  / (1 + sympy.exp(-sS))\n",
    "sympy.integrate(yS * (1 - yS), sS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey!  So, if $f(s)=\\frac{1}{1+e^{-s}}$, the derivative of $f(s)$ is $f(s) (1-f(s))$. \n",
    "\n",
    "Ta-da!  We just arrived at the common sigmoid function used in neural networks. Remembering that $s=\\xv^T \\wv$ is, we get\n",
    "\n",
    "$$\n",
    "f(\\xv; \\wv) = \\frac{1}{1 + e^{-\\xv^T \\wv}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:46.870001Z",
     "start_time": "2022-08-26T17:42:46.591682Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "\n",
    "def df(f_value):\n",
    "    return f_value * (1 - f_value)\n",
    "\n",
    "plt.plot(s, f(s))\n",
    "plt.plot(s, df(f(s)))\n",
    "plt.legend(('$f(s)$', '$\\partial f(s) \\; / \\; \\partial s$'))\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using stochastic gradient descent (SGD) to fit this \"sigmoid\" function to some data.  Find weight values that minimize the sum of squared errors in the output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:56.881434Z",
     "start_time": "2022-08-26T17:42:56.799329Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 20).reshape((-1,1))\n",
    "T = X * 0.1 + np.random.uniform(-0.4, 0.4, size=(20, 1))\n",
    "plt.plot(X, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:42:58.230226Z",
     "start_time": "2022-08-26T17:42:58.056700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = np.zeros((2, 1))\n",
    "rho = 0.1\n",
    "X1 = np.insert(X, 0, 1, 1)\n",
    "for iter in range(100):\n",
    "    for n in range(X.shape[0]):\n",
    "        Xn = X1[n:n + 1, :]\n",
    "        Tn = T[n:n + 1, :]\n",
    "        yn = f(Xn @ w)\n",
    "        w += rho * Xn.T * (Tn - yn) * df(yn) \n",
    "        \n",
    "plt.plot(X, T, 'o')\n",
    "plt.plot(X, f(X1 @ w))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:43:05.030627Z",
     "start_time": "2022-08-26T17:43:04.952645Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 20).reshape((-1, 1))\n",
    "T = 1 + -X * 0.1 + np.random.uniform(-0.2, 0.2, size=(20, 1))\n",
    "plt.plot(X, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:43:05.753655Z",
     "start_time": "2022-08-26T17:43:05.649400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = np.zeros((2, 1))\n",
    "rho = 0.1\n",
    "X1 = np.insert(X, 0, 1, 1)\n",
    "for iter in range(100):\n",
    "    for n in range(X.shape[0]):\n",
    "        Xn = X1[n:n+1, :]\n",
    "        Tn = T[n:n+1, :]\n",
    "        yn = f(Xn @ w)\n",
    "        w += rho * Xn.T * (Tn - yn) * df(yn)\n",
    "        \n",
    "plt.plot(X, T, 'o')\n",
    "plt.plot(X, f(X1 @ w))\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you use this function to fit this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:43:10.877858Z",
     "start_time": "2022-08-26T17:43:10.787961Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 20).reshape((-1, 1))\n",
    "T = X**2 * 0.01 + np.random.uniform(-0.2, 0.2, size=(20, 1))\n",
    "plt.plot(X, T, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:43:12.271105Z",
     "start_time": "2022-08-26T17:43:12.156847Z"
    }
   },
   "outputs": [],
   "source": [
    "w = np.zeros((2, 1))\n",
    "rho = 0.1\n",
    "X1 = np.insert(X, 0, 1, 1)\n",
    "for iter in range(100):\n",
    "    for n in range(X.shape[0]):\n",
    "        Xn = X1[n:n+1, :]\n",
    "        Tn = T[n:n+1, :]\n",
    "        yn = f(Xn @ w)\n",
    "        w += rho * Xn.T * (Tn - yn) * df(yn)\n",
    "        \n",
    "plt.plot(X, T, 'o')\n",
    "plt.plot(X, f(X1 @ w));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right.  We could use two of these functions and add their outputs together.\n",
    "\n",
    "Now we are talking multilayered neural networks!  One layer has two units that output $f(\\xv^T \\wv)$ each with their own $\\wv$.  The second layer has one linear unit with its own $\\wv$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models as Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X$ and $T$, find $\\wv_k$ that minimizes squared error in\n",
    "the $k^{th}$ output, then use it to make predictions.\n",
    "Collect all $\\wv_k$ as columns in $\\Wv$.\n",
    "$\\tilde{\\Xv}$ denotes $\\Xv$ with a column of constant 1's prepended as first column.  The target value\n",
    "for the $k^{th}$ output for the $n^{th}$ sample is $t_{n,k}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E(\\Wv) &= \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - \\tilde{\\xv}_n^T \\wv_k)^2\\\\\n",
    "   \\Wv &= (\\tilde{\\Xv}^T \\tilde{\\Xv})^{-1} \\tilde{\\Xv}^T \\Tv\\\\\n",
    " ~\\\\\n",
    "  \\Wv &= \\begin{bmatrix}\n",
    "    w_{0,1} & w_{0,2} & \\cdots & w_{0,K}\\\\\n",
    "    w_{1,1} & w_{1,2} & \\cdots & w_{1,K}\\\\\n",
    "    \\vdots\\\\\n",
    "    w_{D,1} & w_{D,2} & \\cdots & w_{D,K}\n",
    "  \\end{bmatrix}\\\\\n",
    " ~\\\\\n",
    "  \\Yv &= \\tilde{\\Xv} \\Wv\\\\\n",
    "  ~\\\\\n",
    "\\tilde{\\Xv} & \\text{ is } N \\times (D+1)\\\\\n",
    "\\Wv & \\text{ is } (D+1) \\times K\\\\\n",
    "\\Yv & \\text{ is } N \\times K\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The calculation of $y_{n,k} = \\tilde{\\xv_n} \\wv_k$ can be drawn as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnlinear.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Nonlinear Combinations of Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform $X$ into $\\Phiv(X)$.  For example, \n",
    "\n",
    "$$\n",
    "\\Phiv(X) = \\Phiv\\left ( \n",
    "\\begin{bmatrix}\n",
    "    x_{0,1} & x_{0,2} & \\cdots \\\\\n",
    "    x_{1,1} & x_{1,2} & \\cdots \\\\\n",
    "    \\vdots\\\\\n",
    "    x_{N-1,1} & x_{N-1,2} & \\cdots\n",
    "  \\end{bmatrix}\n",
    "\\right ) = \n",
    "\\begin{bmatrix}\n",
    "    x_{0,1} & x_{0,2}^5  & x_{0,2}^3 x_{0,4}^2 & \\cdots \\\\\n",
    "    x_{1,1} & x_{1,2}^5  & x_{1,2}^3 x_{1,4}^2 & \\cdots\\\\\n",
    "    \\vdots\\\\\n",
    "    x_{N-1,1} & x_{N-1,2}^5 & x_{N-1,2}^3 x_{N-1,4}^2 & \\cdots\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we just replace $\\Xv$ by $\\Phiv$ (which we use to represent\n",
    "$\\Phiv(X)$, and $\\Phiv_n = \\Phiv(\\xv_n)$) and proceed. \n",
    "We do our derivation to minimize\n",
    "\n",
    "$$\n",
    "  E_k = \\sum_{n=1}^N (t_{n,k} - \\tilde\\Phiv_n^T \\wv_k))^2\n",
    "$$\n",
    "\n",
    "from which we find that \n",
    "\n",
    "$$\n",
    "  \\Wv = (\\tilde{\\Phiv}^T \\tilde{\\Phiv})^{-1} \\tilde{\\Phiv}^T T\n",
    "$$\n",
    "\n",
    "and use it like\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\Yv = \\tilde{\\Phiv} \\Wv\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnlinearphi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what should we put in the yellow box?  \n",
    "\n",
    "Can we use the training data to figure this out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Two Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just entered the world of neural networks, with $\\phi(\\xv)$ being the output of another layer of adaptive units, or $\\phi(\\xv) = h(\\xv)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnTwoLayer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\tilde{\\Xv}$ \n",
    "$\\quad\\quad\\quad\\quad\\quad \\Vv $\n",
    "$ \\quad\\quad\\quad\\quad\\quad\\quad\\quad \\tilde{\\Zv} $ \n",
    "$\\quad\\quad\\quad\\quad\\quad \\Wv $ \n",
    "$\\quad\\quad\\quad\\quad \\Yv$\n",
    "\n",
    "$ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad N \\times (D+1)$\n",
    "$\\quad (D+1) \\times M$\n",
    "$ \\quad\\quad\\quad N \\times (M+1)$\n",
    "$\\quad (M+1)\\times K$\n",
    "$ \\quad\\quad N \\times K$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{\\Zv} & = h(\\tilde{\\Xv} \\Vv),\\\\\n",
    "\\Yv & = \\tilde{\\Zv} \\Wv, \\text{ or }\\\\\n",
    "\\Yv & = \\tilde{h}(\\tilde{\\Xv} \\Vv) \\Wv \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The two layers are called the **hidden** and **output** layer.\n",
    "\n",
    "$h$ is the **activation function** for the units in the hidden layer.\n",
    "\n",
    "We will be doing gradient descent in the mean squared error, so want\n",
    "an $h$ whose derivative doesn't grow out of control as $\\vv$ grows, and whose\n",
    "derivative is easy to calculate.\n",
    "\n",
    "What about polynomials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:43:54.633473Z",
     "start_time": "2022-08-26T17:43:54.503691Z"
    }
   },
   "outputs": [],
   "source": [
    "def h(s):\n",
    "    return 3 + 2 * s + 1 * s**2 + 3 * s**5\n",
    "\n",
    "def dh(s):\n",
    "    return 2 + 2 * s + 15 * s**4\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "xs = np.linspace(-5, 5, 100)\n",
    "plt.plot(xs, h(xs), 'b-', linewidth=3, label='$h_1(x)$')\n",
    "plt.plot(xs, dh(xs), 'r-', linewidth=3, label='$\\partial h_1(x) \\; / \\; \\partial x$')\n",
    "plt.grid('on')\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-10000, 10000], 'k-')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gradient descent procedure takes steps of size proportional to the derivative.  This derivative gets huge---high positive as $a$ increases and high negative as $a$ decreases---so gradient descent if very unstable.  Such huge steps will shoot the gradient descent far away from a close local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $s = \\tilde{\\xv}^T \\vv$, two common choices for functions with well-behaved derivatives are the asymmetric sigmoid function we already discussed, and the symmetric sigmoid, often calculated using $\\tanh$.\n",
    "\n",
    "  * sigmoid (asymmetric)\n",
    "  \n",
    "$$\n",
    "        \\begin{align*}\n",
    "          h_1(s) & = \\frac{1}{1+e^{-s}}\n",
    "        \\end{align*}\n",
    "$$\n",
    "\n",
    "  * tanh (symmetric)\n",
    "  \n",
    "$$\n",
    "        \\begin{align*}\n",
    "          h_2(s) & = \\tanh(s) = \\frac{e^{s} - e^{-s}}{e^{s} + e^{-s}}\n",
    "        \\end{align*}\n",
    "$$\n",
    "\n",
    "Work out their derivatives in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:44:04.122363Z",
     "start_time": "2022-08-26T17:44:03.968236Z"
    }
   },
   "outputs": [],
   "source": [
    "def h1(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "\n",
    "def h2(s):\n",
    "    return np.tanh(s)\n",
    "\n",
    "def dh1(s):\n",
    "    y = h1(s)\n",
    "    return y * (1 - y)\n",
    "\n",
    "def dh2(s):\n",
    "    y = h2(s)\n",
    "    return 1 - y*y\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "xs = np.linspace(-5, 5, 100)\n",
    "plt.plot(xs, h1(xs), 'b-', linewidth=3, label='$h_1(s)$')\n",
    "plt.plot(xs, dh1(xs), 'b--', linewidth=3, label='$\\partial h_1(s) \\; / \\; \\partial s$')\n",
    "\n",
    "plt.plot(xs, h2(xs), 'r-', linewidth=3, label='$h_2(s)$')\n",
    "plt.plot(xs, dh2(xs), 'r--', linewidth=3, label='$\\partial h_2(s) \\; / \\; \\partial s$')\n",
    "\n",
    "plt.grid('on')\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-1, 1], 'k-')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training by Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error between each target value $t_{n,k}$ and\n",
    "output (predicted) value $y_{n,k}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E &= \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K ( t_{n,k} - y_{n,k})^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $E$ is no longer a linear function of the parameters (weights), we cannot set the derivative equal to zero and solve for the parameters.  Instead, we can do \n",
    "gradient descent in $E$ by making small changes to weights $v_{j,m}$ and $w_{m,k}$ in the negative gradient direction. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  v_{j,m} &\\leftarrow v_{j,m} - \\rho_h \\frac{\\partial E}{\\partial v_{j,m}}\\\\\n",
    "  w_{m,k} &\\leftarrow w_{m,k} - \\rho_o \\frac{\\partial E}{\\partial w_{m,k}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is often presented as $\\rho_h = \\rho_o$, but having different\n",
    "rates in the two layers often helps convergence rate.\n",
    "\n",
    "Will this find the global optimum (the values of $v$ and $w$\n",
    "that minimize the mean squared error)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First a Simplified View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our picture of a full two-layer network?  Let's focus\n",
    "on how to modify a single weight, $v_{1,1}$, based on a single error between $y_1$ and $t_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnTwoLayerTargetsOnePath.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the subscripts for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnOnePath.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward calculation (ignoring constant 1 input and all other terms)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  y &= w\\; h(v\\;x) \\text{ or }\\\\\n",
    "  y &= w\\; z\\\\\n",
    "  z &= h(a)\\\\\n",
    "  a &= v\\; x\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnOnePath2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $E = (t - y)^2$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial E}{\\partial v} & = \\frac{\\partial (t - y)^2}{\\partial v}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The Chain Rule to the rescue.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial E}{\\partial v} & = \\frac{\\partial (t-y)^2}{\\partial v}\\\\\n",
    "  & = \\frac{\\partial (t-y)^2}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial a} \\frac{\\partial a}{\\partial v} \\\\\n",
    "  & = 2(t-y) \\;\\;(-1)\\;\\; w \\;\\;  \\frac{\\partial h(a)}{\\partial a} \\;\\; x \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If $h(a) = \\tanh(a)$, then $\\frac{\\partial h(a)}{\\partial a} = (1-h(a)^2) = (1-z^2)$. See [this page at Mathematics Stack Exchange](http://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial E}{\\partial v} & = -2(t-y) \\;\\; w \\;\\;  \\frac{\\partial h(a)}{\\partial a} \\;\\; x \\\\\n",
    "  & = -2(t-y) \\;\\; w \\;\\;  (1-z^2) \\;\\; x\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's add another output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnTwoPath.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chain Rule again.\n",
    "\n",
    "$$\n",
    "       \\begin{align*}\n",
    "       \\frac{\\partial E}{\\partial v} & = \\frac{\\partial (t_1-y_1)^2 + (t_2-y_2)^2}{\\partial v}\\\\\n",
    "       & = \\frac{\\partial (t_1-y_1)^2 + (t_2-y_2)^2}{\\partial z} \\frac{\\partial z}{\\partial a} \\frac{\\partial a}{\\partial v} \\\\\n",
    "       & = \\left ( \\frac{\\partial (t_1-y_1)^2 + (t_2-y_2)^2}{\\partial y_1}\\frac{\\partial y_1}{\\partial z} + \\frac{\\partial (t_1-y_1)^2 + (t_2-y_2)^2}{\\partial y_2}\\frac{\\partial\n",
    "           y_2}{\\partial z} \\right )  \\frac{\\partial z}{\\partial a} \\frac{\\partial a}{\\partial v} \\\\\n",
    "       & = \\left ( -2(t_1-y_1) w_1 - 2(t_2 -y_2) w_2 \\right ) \\frac{\\partial z}{\\partial a} \\frac{\\partial a}{\\partial v} \\\\\n",
    "       & = -2 \\left ( (t_1-y_1) w_1 + (t_2-y_2) w_2 \\right ) \\frac{\\partial h(a)}{\\partial a} x \\\\\n",
    "       & = -2 \\left ( (t_1-y_1) w_1 + (t_2-y_2) w_2 \\right ) (1-z^2) x \n",
    "       \\end{align*}\n",
    "$$\n",
    "\n",
    "Think of the errors calculated in the output units as being sent backwards to the units in the previous layer.  If we label these errors as \"delta\" values, our derivative expressions form what are commonly referred to as \"delta rules\".  The delta values are \"backpropagated\" to the previous layer.  This process is called \"error backpropagation\", but is really just the chain rule of calculating derivatives!\n",
    "\n",
    "First, remember these derivatives.\n",
    "\n",
    "$$\n",
    "       \\begin{align*}\n",
    "       \\frac{\\partial E}{\\partial v} & = -2 ( (t_1-y_1) w_1 + (t_2-y_2) w_2  ) (1-z^2) x \\\\\n",
    "       \\frac{\\partial E}{\\partial w_1} & = -2 (t_1-y_1) z\n",
    "       \\end{align*}\n",
    "$$\n",
    "\n",
    "Now the update rules involving the deltas ($\\delta$) are \n",
    "\n",
    "$$\n",
    "       \\begin{align*}\n",
    "       w &\\leftarrow w - \\frac{\\partial E}{\\partial w_1}\\\\\n",
    "       &\\leftarrow w + \\rho_o  (t_1-y_1) z\\\\\n",
    "       &\\leftarrow w + \\rho_o  \\delta_1^{(o)}  z\\\\\n",
    "       v & \\leftarrow v - \\frac{\\partial E}{\\partial v}\\\\\n",
    "       & \\leftarrow v + \\rho_h \\left ( (t_1-y_1) w_1 + (t_2-y_2) w_2 \\right ) (1-z^2) x \\\\\n",
    "       & \\leftarrow v + \\rho_h \\left ( (t_1-y_1) w_1 + (t_2-y_2) w_2 \\right ) (1-z^2) x \\\\\n",
    "       & \\leftarrow v + \\rho_h \\left ( \\delta_1^{(o)} w_1 + \\delta_2^{(o)} w_2 \\right ) (1-z^2) x \\\\\n",
    "       & \\leftarrow v + \\rho_h \\delta^{(h)} x\\\\\n",
    "&\\;\\;\\; \\text{ where } \\delta^{(h)} =  ( \\delta_1^{(o)} w_1 + \\delta_2^{(o)} w_2  ) (1-z^2)\n",
    "       \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the Full Version of Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/nnTwoLayerBackProp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "  v_{j,m} &\\leftarrow v_{j,m} - \\rho_h \\frac{\\partial E}{\\partial\n",
    "    v_{j,m}}\\\\\n",
    " w_{m,k} & \\leftarrow w_{m,k} - \\rho_o \\frac{\\partial E}{\\partial w_{m,k}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "A bit of calculus and algebra lead us to these expressions for doing gradient descent.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  v_{j,m} &\\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k}) w_{m,k} (1-\\tilde{z}_m^2) \\tilde{x}_{n,j}\\\\\n",
    "  w_{m,k} &\\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK} \\sum_{n=1}^N\n",
    "  (t_{n,k} - y_{n,k}) \\tilde{z}_m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now we will derive this result.\n",
    "First work on $\\frac{\\partial E}{\\partial w_{m,k}}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E &= \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})^2\\\\\n",
    "  \\frac{\\partial E}{\\partial w_{m,k}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \\frac{\\partial y_{n,k'}}{\\partial w_{m,k}}\\\\\n",
    "  \\text{Since } & y_{n,k'} = \\sum_{m'=0}^M  w_{m',k'} \\tilde{z}_{n,m'}\\\\\n",
    "  \\frac{\\partial E}{\\partial w_{m,k}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \n",
    "  \\frac{\\partial \\left ( \\sum_{m'=0}^M w_{m',k'} \\tilde{z}_{n,m'}  \\right )}{\\partial w_{m,k}}\\\\\n",
    "  & = -2 \\frac{1}{NK} \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now the hard one,  $\\frac{\\partial E}{\\partial v_{j,m}}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  E &= \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})^2\\\\\n",
    "  \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \\frac{\\partial y_{n,k'}}{\\partial  v_{j,m}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Knowing that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  y_{n,k'} & = \\sum_{m'=0}^M  w_{m',k'} \\tilde{z}_{n,m'}\\\\\n",
    "  &  = \\sum_{m'=0}^M  w_{m',k'} \\tilde{h}\\left ( \\sum_{j'=0}^D\n",
    "    v_{j,m'} \\tilde{x}_{n,j'} \\right )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "we can continue.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'}) \n",
    "  \\frac{\\partial \\left ( \\sum_{m'=0}^M  w_{m',k'} \\tilde{h} \\left ( \\sum_{j'=0}^D v_{j',m'} \\tilde{x}_{n,j'} \\right ) \\right )}{\\partial v_{j,m}}\\\\ \n",
    "  \\text{Let } a_{n,m'} & = \\tilde{h} \\left ( \\sum_{j'=0}^D v_{j',m'} \\tilde{x}_{n,j'}  \\right )\\\\\n",
    "  \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'})  \\sum_{m'=0}^M  w_{m',k'} \n",
    "  \\frac{\\partial \\tilde{h}(a_{n,m'})}{\\partial  a_{n,m'}} \\frac{\\partial a_{n,m'}}{\\partial v_{j,m}}\\\\\n",
    "  & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k'=1}^K (t_{n,k'} - y_{n,k'})  \\sum_{m'=0}^M  w_{m',k'} \\frac{\\partial \\tilde{h}(a_{n,m'})}{\\partial a_{n,m'}} \\tilde{x}_{n,j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To summarize:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      E &= \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})^2\\\\\n",
    "      \\frac{\\partial E}{\\partial w_{m,k}} & = -2 \\frac{1}{N}  \\frac{1}{K}\n",
    "       \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\\\\\n",
    "      \\frac{\\partial E}{\\partial v_{j,m}} & = -2 \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Forward pass\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      z_{n,m} &= h(\\sum_{j=0}^D v_{j,m} \\tilde{x}_{n,j})\\\\\n",
    "      y_{n,k} &= \\sum_{m=1}^M w_{m,k} \\tilde{z}_{n,m}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Backward pass\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\\\\\n",
    " w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK}  \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Backward pass with $\\delta$ terms\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\delta_{n,k}^w &= t_{n,k} - y_{n,k}\\\\\n",
    "      \\delta_{n,m}^v &= \\sum_{k=1}^K \\delta_{n,k}^w w_{m,k} (1 - z_{n,m}^2)\\\\\n",
    "\\\\\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\\\\\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N \\delta_{n,m}^v \\tilde{x}_{n,j}\\\\\n",
    "\\\\\n",
    " w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK}  \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\\\\\n",
    " w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK} \\sum_{n=1}^N \\delta_{n,k}^w \\tilde{z}_{n,m}\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Convert these scalar expressions to matrix expressions.\n",
    "\n",
    "Forward Pass\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      z_{n,m} &= h(\\sum_{j=0}^D v_{j,m} \\tilde{x}_{n,j})\\\\\n",
    "      z_{n,m} &= h( v_{*,m} \\tilde{x}_{n,*})\\\\\n",
    "      z_{n,m} &= h(  \\tilde{x}_{n,*} v_{*,m})\\\\\n",
    "      z_{*_n,m} &= h(  \\tilde{x}_{*_n,*} v_{*,m})\\\\\n",
    "      z_{*_n,*_m} &= h(  \\tilde{x}_{*_n,*} v_{*,*_m})\\\\\n",
    "      \\Zv &= h(\\tilde{\\Xv} \\Vv)\\\\\n",
    "      y_{n,k} &= \\sum_{m=1}^M w_{m,k} \\tilde{z}_{n,m}\\\\\n",
    "      y_{n,k} &=  \\tilde{z}_{n,*} w_{*,k}\\\\\n",
    "      y_{*_n,*_k} &=  \\tilde{z}_{*_n,*} w_{*,*_k}\\\\\n",
    "      \\Yv &= \\tilde{\\Zv} \\Wv\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Backward Pass for $\\Wv$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK} \\sum_{n=1}^N (t_{n,k} - y_{n,k}) \\tilde{z}_{n,m}\\\\\n",
    "      & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK} (t_{*_n,k} - y_{*_n,k})^T \\tilde{z}_{*_n,m}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Almost.  Check the shapes of left and right hand sides of last equation. Result on right hand side has subscripts $k,m$ but left side is $m,k$. So\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      w_{m,k} & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK}   \\left ( (t_{*_n,k} - y_{*_n,k})^T \\tilde{z}_{*_n,m} \\right )^T\\\\\n",
    "       & \\leftarrow w_{m,k} + \\rho_o \\frac{1}{NK} \\tilde{z}_{*_n,m}^T (t_{*_n,k} - y_{*_n,k}) \\\\\n",
    "      w_{*_m,*_k} & \\leftarrow w_{*_m,*_k} + \\rho_o \\frac{1}{NK}  \\tilde{z}_{*_n,*_m}^T (t_{*_n,*_k} - y_{*_n,*_k}) \\\\\n",
    "      \\Wv & \\leftarrow \\Wv + \\rho_o \\frac{1}{NK}  \\tilde{\\Zv}^T (\\Tv - \\Yv)\\\\\n",
    "      \\Wv & \\leftarrow \\Wv + \\rho_o \\frac{1}{NK}  \\tilde{\\Zv}^T \\dimensionbar{M+1 \\times N} (\\Tv - \\Yv \\dimensionbar{N\\times K}) \\dimensionbar{M+1 \\times K}\n",
    "    \\end{align*}\n",
    "$$\n",
    "Or, with the $\\delta$ terms we have\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      w_{*_m,*_k} & \\leftarrow w_{*_m,*_k} + \\rho_o \\frac{1}{NK}  \\tilde{z}_{*_n,*_m}^T (t_{*_n,*_k} - y_{*_n,*_k}) \\\\\n",
    "      w_{*_m,*_k} & \\leftarrow w_{*_m,*_k} + \\rho_o \\frac{1}{NK}  \\tilde{z}_{*_n,*_m}^T \\delta^w_{*_n, *_k} \\\\\n",
    "      \\Dv^w &= \\delta^w_{*_n, *_k}\\\\\n",
    "      \\Dv^w &= T - Y\\\\\n",
    "      \\Wv & \\leftarrow \\Wv + \\rho_o \\frac{1}{NK}  \\tilde{\\Zv}^T \\Dv^w\\\\\n",
    "      \\Wv & \\leftarrow \\Wv + \\rho_o \\frac{1}{NK}  \\tilde{\\Zv}^T \\dimensionbar{M+1 \\times N} \\Dv^w \\dimensionbar{N\\times K}) \\dimensionbar{M+1 \\times K}\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Pass for $\\Vv$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N \\sum_{k=1}^K (t_{n,k} - y_{n,k})   w_{m,k} (1-z_{n,m}^2) \\tilde{x}_{n,j}\\\\\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\sum_{n=1}^N  (t_{n,*_k} - y_{n,*_k}) w_{m,*_k}^T   (1-z_{n,m}^2) \\tilde{x}_{n,j}\\\\\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{m,*_k}^T \\cdot  (1-z_{*_n,m}^2) \\dimensionbar{N\\times m} \\right )^T\n",
    "      \\tilde{x}_{*_n,j} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Almost.  Result on right hand side has subscripts $m,j$, but left side is $j,m$. So\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\left ( \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{m,*_k}^T \\cdot  (1-z_{*_n,m}^2) \\right )^T\n",
    "      \\tilde{x}_{*_n,j} \\right )^T\\\\\n",
    "       & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\tilde{x}_{*_n,j}^T \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{m,*_k}^T \\cdot  (1-z_{*_n,m}^2) \\right )\\\\\n",
    "       v_{*_j,*_m} & \\leftarrow v_{*_j,*_m} + \\rho_h \\frac{1}{NK} \\tilde{x}_{*_n,*_j}^T \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{*_m,*_k}^T \\cdot  (1-z_{*_n,*_m}^2)\n",
    "       \\right )\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} \\tilde{\\Xv}^T \\left ( (\\Tv - \\Yv) \\hat{\\Wv}^T \\cdot  (1-\\Zv^2) \\right )\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} \\tilde{\\Xv}^T \\dimensionbar{D+1 \\times N} \\left ( (\\Tv - \\Yv) \\dimensionbar{N\\times K} \\hat{\\Wv}^T \\dimensionbar{N\\times M} \\cdot  (1-\\Zv^2)\n",
    "         \\dimensionbar{N\\times M}\\right ) \\dimensionbar{D+1 \\times M}\\\\\n",
    "       \\text{where } & \\hat{\\Wv} \\text{ is } \\Wv \\text{ without constant input row}\n",
    "\\end{align*}\n",
    "$$\n",
    "Or, again, with the $\\delta$ terms we have\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      v_{j,m} & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\left ( \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{m,*_k}^T \\cdot  (1-z_{*_n,m}^2) \\right )^T\n",
    "      \\tilde{x}_{*_n,j} \\right )^T\\\\\n",
    "       & \\leftarrow v_{j,m} + \\rho_h \\frac{1}{NK} \\tilde{x}_{*_n,j}^T \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{m,*_k}^T \\cdot  (1-z_{*_n,m}^2) \\right )\\\\\n",
    "       v_{*_j,*_m} & \\leftarrow v_{*_j,*_m} + \\rho_h \\frac{1}{NK} \\tilde{x}_{*_n,*_j}^T \\left ( (t_{*_n,*_k} - y_{*_n,*_k}) w_{*_m,*_k}^T \\cdot  (1-z_{*_n,*_m}^2)\n",
    "       \\right )\\\\\n",
    "      \\delta_{n,m}^v &= \\sum_{k=1}^K \\delta_{n,k}^w w_{m,k} (1 - z_{n,m}^2)\\\\\n",
    "      \\delta_{n,m}^v &= \\delta_{n,*_k}^w w_{m,*_k}^T (1 - z_{n,m}^2)\\\\\n",
    "      \\delta_{*_n,m}^v &= \\delta_{*_n,*_k}^w w_{m,*_k}^T (1 - z_{*_n,m}^2)\\\\\n",
    "      \\Dv^v &= \\Dv^w \\hat{\\Wv}^T \\cdot (1 - \\Zv^2)\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} \\tilde{\\Xv}^T \\left ( (\\Tv - \\Yv) \\hat{\\Wv}^T \\cdot  (1-\\Zv^2) \\right )\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} \\tilde{\\Xv}^T \\Dv^v\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} ( \\tilde{\\Xv}^T \\dimensionbar{D+1 \\times N} \\Dv^v \\dimensionbar{N\\times M} ) \\dimensionbar{D+1 \\times M}\\\\\n",
    "       \\text{where } & \\hat{\\Wv} \\text{ is } \\Wv \\text{ without constant input row}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "All together now, first in math.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\Zv &= h(\\tilde{\\Xv} \\Vv)\\\\\n",
    "      \\Yv &= \\tilde{\\Zv} \\Wv\\\\\n",
    "      \\Dv^w &= \\Tv - \\Yv\\\\\n",
    "      \\Dv^v &= \\Dv^w \\hat{\\Wv}^T \\cdot (1 - \\Zv^2)\\\\\n",
    "      \\Wv & \\leftarrow \\Wv + \\rho_o \\frac{1}{NK}\\tilde{\\Zv}^T \\Dv^w\\\\\n",
    "       \\Vv & \\leftarrow \\Vv + \\rho_h \\frac{1}{NK} \\tilde{\\Xv}^T \\Dv^v\\\\\n",
    "      \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in python.\n",
    "\n",
    "    # Given input X and target T, do forward pass to calculate network output, Y\n",
    "    ro = ro / (N * K)\n",
    "    rh = rh / (N * K)\n",
    "    X1 = addOnes(X)\n",
    "    Z = tanh(X1 @ V))\n",
    "    Z1 = addOnes(Z)\n",
    "    Y = Z1 @ W\n",
    "    # Do gradient descent on derivative of squared error with respect to each weight to update V and W.\n",
    "    Dw = T - Y\n",
    "    Dv = Dw @ W[1:, :].T * (1 - Z**2)\n",
    "    W = W + ro * Z1.T @ Dw\n",
    "    V = V + rh * X1.T @ Dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equations show a single step down the gradient of our mean square error, $E$. Here we are using all of our training data, in $X$ and $T$ to calculate the gradient of $E$ with respect to the weights.  Instead of writing  a for loop to step through each sample and update the weights on each step, which we called (Stochastic Gradient Descent), we are using the full gradient.\n",
    "\n",
    "Here is an example. Let's fit a neural network to the function\n",
    "\n",
    "$$\n",
    "f(x) = 0.2 + 0.05 (x+10) + 0.4 \\sin(x+10)) + 0.2 \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a random variable drawn from the standard normal\n",
    "distribution, or $\\epsilon \\sim \\mathcal{N}(0,1)$, and $x \\in [-10,10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:45:58.702844Z",
     "start_time": "2022-08-26T17:45:58.700575Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "import time  # for sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:46:05.735414Z",
     "start_time": "2022-08-26T17:46:05.732564Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make some training data\n",
    "n = 30\n",
    "Xtrain = np.linspace(0., 20.0, n).reshape((n, 1)) - 10\n",
    "Ttrain = 0.2 + 0.05 * (Xtrain + 10) + 0.4 * np.sin(Xtrain + 10) + 0.2 * np.random.normal(size=(n, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have worked on reducing the error on the data we have as much as possible.  Really what we want from our machine learning models is models that generalize well, meaning that they produce small errors on new data that was not used to train our model.  So, let's make some new, testing, data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:46:07.903049Z",
     "start_time": "2022-08-26T17:46:07.900086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make some testing data\n",
    "Xtest = Xtrain + 0.1 * np.random.normal(size=(n, 1))\n",
    "Ttest = 0.2 + 0.05 * (Xtest + 10) + 0.4 * np.sin(Xtest + 10) + 0.2 * np.random.normal(size=(n, 1))\n",
    "\n",
    "nSamples = Xtrain.shape[0]\n",
    "nOutputs = Ttrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:02:56.281698Z",
     "start_time": "2022-08-26T18:02:56.278710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add constant column of 1's\n",
    "def add_ones(A):\n",
    "    return np.insert(A, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:02:56.723101Z",
     "start_time": "2022-08-26T18:02:56.719326Z"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.uniform(-1, 1, size=(5, 3))  # 5 samples, each having 3 components\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:02:57.373651Z",
     "start_time": "2022-08-26T18:02:57.369914Z"
    }
   },
   "outputs": [],
   "source": [
    "add_ones(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need an RMSE function that also unstandardizes the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:56:32.107945Z",
     "start_time": "2022-08-26T17:56:32.100830Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(T, Y, Tstds):\n",
    "    error = (T - Y) * Tstds \n",
    "    return np.sqrt(np.mean(error ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardize the inputs and targets. This can help with the selection of learning rates.\n",
    "\n",
    "To standardize the inputs in $X$, subtract the column means and divide the result by the column standard deviations.  Do the same for the target values in $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T17:56:35.596890Z",
     "start_time": "2022-08-26T17:56:35.577506Z"
    }
   },
   "outputs": [],
   "source": [
    "Xmeans = Xtrain.mean(axis=0)\n",
    "Xstds = Xtrain.std(axis=0)\n",
    "Tmeans = Ttrain.mean(axis=0)\n",
    "Tstds = Ttrain.std(axis=0)\n",
    "\n",
    "XtrainS = (Xtrain - Xmeans) / Xstds\n",
    "TtrainS = (Ttrain - Tmeans) / Tstds\n",
    "XtestS = (Xtest - Xmeans) / Xstds\n",
    "TtestS = (Ttest - Tmeans) / Tstds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to prepare for our matrix operations, prepend the column of constant ones to $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:03:47.685837Z",
     "start_time": "2022-08-26T18:03:47.682928Z"
    }
   },
   "outputs": [],
   "source": [
    "XtrainS1 = add_ones(XtrainS)\n",
    "XtestS1 = add_ones(XtestS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must initialize the weights to random values. If all weights are initialized to zero, all hidden units will learn identical weights, as if we only have one hidden unit.  Why is this so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-26T18:16:01.021638Z",
     "start_time": "2022-08-26T18:15:36.163378Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Set parameters of neural network\n",
    "n_hiddens = 20\n",
    "\n",
    "n_samples, n_outputs = Ttrain.shape\n",
    "\n",
    "rho_h = 0.5\n",
    "rho_o = 0.1\n",
    "\n",
    "rho_h = rho_h / (n_samples * n_outputs)\n",
    "rho_o = rho_o / (n_samples * n_outputs)\n",
    "\n",
    "# Initialize weights to uniformly distributed values between small normally-distributed between -0.1 and 0.1\n",
    "V = np.random.uniform(-1, 1, size=(1 + 1, n_hiddens)) / np.sqrt(XtrainS1.shape[1])\n",
    "W = np.random.uniform(-1, 1, size=(1 + n_hiddens, n_outputs)) / np.sqrt(n_hiddens + 1)\n",
    "\n",
    "# Take n_epochs steepest descent steps in gradient descent search in mean-squared-error function\n",
    "n_epochs = 100000\n",
    "\n",
    "# collect training and testing errors for plotting\n",
    "error_trace = []\n",
    "\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Function we wish to minimize, mean squared error\n",
    "    # ------------------------------------------------\n",
    "    # Forward pass on all training data\n",
    "    Z = np.tanh(XtrainS1 @ V)\n",
    "    Z1 = add_ones(Z)\n",
    "    Y = Z1 @ W\n",
    "    mse = np.mean((TtrainS - Y)**2)\n",
    "    \n",
    "    # Gradient of mean squared error with respect to V and W\n",
    "    # ------------------------------------------------------\n",
    "    Dw = TtrainS - Y\n",
    "    Dv = Dw @ W[1:, :].T * (1 - Z**2)\n",
    "    grad_wrt_W = - Z1.T @ Dw\n",
    "    grad_wrt_V = - XtrainS1.T @ Dv\n",
    "    \n",
    "    # Take step down the gradient\n",
    "    W = W - rho_o * grad_wrt_W\n",
    "    V = V - rho_h * grad_wrt_V  \n",
    "\n",
    "    # Apply model with new weights to train and test data, calculate the RMSEs and append to error_trace\n",
    "    YtrainS = add_ones(np.tanh(XtrainS1 @ V)) @ W    # Forward pass in one line !!\n",
    "    YtestS = add_ones(np.tanh(XtestS1 @ V)) @ W \n",
    "    error_trace.append([rmse(TtrainS, YtrainS, Tstds),\n",
    "                        rmse(TtestS, YtestS, Tstds)])\n",
    "    \n",
    "    if epoch % 2000 == 0 or epoch == n_epochs - 1:\n",
    "        \n",
    "        with plt.rc_context({'font.size': 15}):\n",
    "            plt.clf()\n",
    "            plt.subplot(3, 1, 1)\n",
    "            plt.plot(np.array(error_trace)[:epoch, :])\n",
    "            plt.ylim(0, 0.7)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.legend(('Train','Test'), loc='upper left')\n",
    "\n",
    "            plt.subplot(3, 1, 2)\n",
    "            Ytest = YtestS * Tstds + Tmeans\n",
    "            plt.plot(Xtrain, Ttrain, 'o-', label='Train Data')\n",
    "            plt.plot(Xtest, Ttest, 'o-', label='Test Data')\n",
    "            plt.plot(Xtest, Ytest, 'o-', label='Model Output')\n",
    "            plt.xlim(-10, 10)\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.xlabel('$x$')\n",
    "            plt.ylabel('Actual and Predicted $f(x)$')\n",
    "\n",
    "            plt.subplot(3, 1, 3)\n",
    "            plt.plot(Xtrain, Z)\n",
    "            plt.ylim(-1.1, 1.1)\n",
    "            plt.xlabel('$x$')\n",
    "            plt.ylabel('Hidden Unit Outputs ($z$)');\n",
    "\n",
    "            ipd.clear_output(wait=True)\n",
    "            ipd.display(fig)\n",
    "        \n",
    "ipd.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
